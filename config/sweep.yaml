# sweep.yaml
method: grid
metric:
  name: train/reward
  goal: maximize
parameters:
  alpha:
    values: [0.1, 0.2, 0.3, 0.4] #[0.5, 0.3, 0.1, 0.7, 0.8] #[0.9, 1] # example alpha values
  learning_rate:
    values: [0.1] #[1e-2, 1e-1, 1e-3, 1e-4] #[ 1e-3,1e-14, 1e-16, 1e-2, 1e-12, 1e-11 1e-8, 1e-9, 1e-10, s1e-5,1e-7, 0.0001, 0.001, 0.00001]
  discount_factor:
    values: [0.9]
  exploration_rate:
    values: [1.0]
  exploration_decay_rate:
    values: [0.00001]
  min_epsilon:
    values: [0.05]
  learning_rate_decay:
    values: [0.99995]
  min_learning_rate:
    values: [0.001]
  target_network_update_frequency:
    values: [50, 100] #[100,10]
  max_episodes:
    values: [3000]
  train_frequency:
    values: [1]

## sweep_random.yaml
#method: random
#metric:
#  name: average_return
#  goal: maximize
#parameters:
#  alpha:
#    distribution: uniform
#    min: 0.1
#    max: 0.99
#  learning_rate:
#    distribution: uniform
#    min: 0.0001
#    max: 0.1
#  discount_factor:
#    distribution: uniform
#    min: 0.5
#    max: 0.99
#  exploration_rate:
#    distribution: uniform
#    min: 0.1
#    max: 1.0
#  exploration_decay_rate:
#    distribution: uniform
#    min: 0.8
#    max: 0.998
#  min_exploration_rate:
#    distribution: uniform
#    min: 0.00000001
#    max: 0.00001

